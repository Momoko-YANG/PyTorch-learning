{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0863e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc185f",
   "metadata": {},
   "source": [
    "#### Autograd求导机制\n",
    "PyTorch中所有神经网络的核心是`autograd`包。\n",
    "`autograd`包为张量上的所有操作提供了自动求导。它是一个在运行时定义的框架，这意味这反向传播时根据代码来确定如何运行，并且每次迭代可以是不同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7616e",
   "metadata": {},
   "source": [
    "#### 张量（Tesnor）\n",
    "`torch.Tensor`是这个包的核心类。如果设置`.requires_grad`为`True`，那么将会追踪所有对于该张量的操作。当完成计算后通过调用`.backward()`，自动计算所有的梯度，这个张量的所有梯度将会自动积累到`.grad`属性。\n",
    "\n",
    "要阻止张量跟踪历史记录，可以调用`.detach()`方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。\n",
    "\n",
    "为了防止跟踪历史记录（和使用内存），可以将代码块包装在`with torch.no_grad():`中。在评估模型时特别有用，因为模型可能具有`requires_grad = True`的可训练参数，但是我们不需要梯度计算。\n",
    "\n",
    "在自动梯度计算中还有另外一个重要的类`Function`。\n",
    "\n",
    "`Tensor`和`Function`互相连接并生成一个非循环图，它表示和存储了完整的计算历史，每个张量都有一个`.grad_fn`属性，这个属性引用了一个创建了`Tensor`的`Function`（除非这个张量时用户手动创建的，即这个张量的`grad_fn`是`None`）\n",
    "\n",
    "如果需要计算导数，你可以在`Tensor`上调用`.backward()`。如果`Tensor`是一个标量（即它包含一个元素数据）则不需要为`backward()`指定任何参数。到那时如果它有更多的元素，你需要指定一个`gradient`参数来匹配张量的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ad61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e402bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True) # 创建一个2x2的张量，并设置requires_grad=True以便进行梯度计算\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab872184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87653590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x109be6ac0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)  # y是通过一个加法操作创建的，所以它有一个grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a102755e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out) # z是通过乘法和加法操作创建的，out是z的均值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76baca",
   "metadata": {},
   "source": [
    "`.requires_grad_(...)`可以改变现有张量的`requires_grad`属性。如果没有指定的话，默认输入的flag是`False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69394bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x109be6ac0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)  # 默认情况下，requires_grad是False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)  # 现在requires_grad是True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)  # b是通过一系列操作创建的，所以它有一个grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9c743",
   "metadata": {},
   "source": [
    "#### 梯度\n",
    "反向传播\n",
    "因为`out`是一个标量（scalar），`out.backward()`等于`out.backward(torch.tensor(1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42374fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()  # 反向传播，计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9477ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)  # 输出x的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c1f30",
   "metadata": {},
   "source": [
    "来看一个vector-Jacobian product的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21fb6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([282.7868, 976.9248, 272.6789], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True) # 创建一个3维的张量，并设置requires_grad=True\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000: # 当y的范数小于1000时，继续循环\n",
    "    y = y * 2\n",
    "\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a76bd",
   "metadata": {},
   "source": [
    "这种情形中，`y`不再是标量。`torch.autograd`无法直接计算出完整的雅可比行列，但是如果我们只想要vector-Jacobian product，只需要将向量作为参数传入`backward`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63a53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)  # 定义一个梯度向量\n",
    "y.backward(gradients)  # 计算vector-Jacobian product\n",
    "\n",
    "print(x.grad)  # 输出x的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383558d5",
   "metadata": {},
   "source": [
    "如果`.requires_grad=True`但是又不希望进行autograd的计算，那么可以将变量包裹在`with torch.no_grad()`中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a455697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)  # 检查x是否需要梯度计算\n",
    "print((x ** 2).requires_grad)  # 检查x的平方是否需要梯度计算\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)  # 在no_grad环境下，x的平方不需要梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93143191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
