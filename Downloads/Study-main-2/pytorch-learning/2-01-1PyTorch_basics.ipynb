{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89a554e",
   "metadata": {},
   "source": [
    "#### PyTorch基础：张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dcde8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550ecd9",
   "metadata": {},
   "source": [
    "#### 张量（Tensor）\n",
    "Tensor是PyTorch里面基础的运算单位，与Numpy的ndarray相同，都表示的是一个多维的矩阵。与ndarray的最大区别就是，PyTorch的Tensor可以在GPU上运行，而Numpy的ndarray只能在CPU上运行，在GPU上运行大大加快了运算速度。\n",
    "\n",
    "下面我们生成一个简单的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a27aa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7641, 0.2276, 0.7648],\n",
       "        [0.0197, 0.9050, 0.5619]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3) # 生成一个2行3列的随机张量\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e74489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)  # 输出张量的形状\n",
    "print(x.size())  # 输出张量的形状，等价于x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063900e",
   "metadata": {},
   "source": [
    "张量（tensor）是一个定义在一些张量空间和一些对偶空间的笛卡尔积上的多重线性映射，其坐标是n维空间内，有n个分量的一种量，其中每个分量都是坐标的函数。而在坐标变换时，这些分量也依照某些规则作线性变换。r称为该张量的秩或阶（与矩阵的秩和阶均无关系）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f639dfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9627, 0.1141, 0.8486, 0.8356, 0.8587],\n",
       "          [0.5418, 0.8174, 0.6005, 0.0965, 0.8303],\n",
       "          [0.1391, 0.3076, 0.7895, 0.6155, 0.6628],\n",
       "          [0.3231, 0.5223, 0.5444, 0.8989, 0.4116]],\n",
       "\n",
       "         [[0.3931, 0.8159, 0.7216, 0.1741, 0.6903],\n",
       "          [0.8077, 0.0810, 0.6242, 0.7658, 0.6776],\n",
       "          [0.9745, 0.5951, 0.5533, 0.6079, 0.3307],\n",
       "          [0.7373, 0.2226, 0.8817, 0.2303, 0.9428]],\n",
       "\n",
       "         [[0.9879, 0.7443, 0.7721, 0.0296, 0.6624],\n",
       "          [0.3084, 0.1969, 0.3693, 0.1537, 0.1498],\n",
       "          [0.1185, 0.4198, 0.9368, 0.7222, 0.7226],\n",
       "          [0.6109, 0.5037, 0.3604, 0.0631, 0.8926]]],\n",
       "\n",
       "\n",
       "        [[[0.1489, 0.3011, 0.9575, 0.2981, 0.1518],\n",
       "          [0.9751, 0.5308, 0.8342, 0.2133, 0.4051],\n",
       "          [0.9869, 0.1186, 0.4335, 0.0045, 0.6526],\n",
       "          [0.3630, 0.0477, 0.9128, 0.6366, 0.6189]],\n",
       "\n",
       "         [[0.1986, 0.3052, 0.9606, 0.3996, 0.4836],\n",
       "          [0.4570, 0.1778, 0.9988, 0.1658, 0.7575],\n",
       "          [0.7717, 0.8957, 0.0051, 0.0272, 0.0636],\n",
       "          [0.5142, 0.8078, 0.3917, 0.5678, 0.8219]],\n",
       "\n",
       "         [[0.7913, 0.1670, 0.1333, 0.0792, 0.6229],\n",
       "          [0.7346, 0.3928, 0.4827, 0.0359, 0.7416],\n",
       "          [0.6001, 0.6131, 0.1578, 0.6034, 0.6070],\n",
       "          [0.3516, 0.2786, 0.6116, 0.0037, 0.6802]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(2,3,4,5)  # 生成一个2x3x4x5的随机张量\n",
    "print(y.size())  # 输出张量的形状\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bebaff",
   "metadata": {},
   "source": [
    "在同构的意义下，第零阶张量（r=0）为标量（Scalar），第一阶张量（r=1）为向量（Vector），第二阶张量（r=2）则为矩阵（Matrix），第三阶以上统称为多维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed71b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = torch.tensor(5)  # 标量\n",
    "print(scalar)\n",
    "scalar.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33f3e9",
   "metadata": {},
   "source": [
    " 对于标量，我们可以直接用`.item()`从中取出其对应的python对象的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6882fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()  # 取出标量的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de2cb88",
   "metadata": {},
   "source": [
    "特别的：如果张量中只有一个元素的tensor也可以调用`tensor.item`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55ee99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([42])  # 只有一个元素的张量\n",
    "print(tensor)\n",
    "tensor.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e537a46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.item()  # 取出张量中唯一元素的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4a66c",
   "metadata": {},
   "source": [
    "#### 基本类型\n",
    "\n",
    "Tensor的基本数据类型有五种：\n",
    "\n",
    "- 32位浮点型：`torch.FloatTensor`（默认）\n",
    "- 64位整型：`torch.LongTensor`\n",
    "- 32位整型：`torch.IntTensor`\n",
    "- 16位整型：`torch.ShortTensor`\n",
    "- 64位浮点型：`torch.DoubleTensor`\n",
    "\n",
    "除以上数字类型外，还有`byte`和 `chart`型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1724fe4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long = tensor.long()  # 转换为长整型张量\n",
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f26b1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.], dtype=torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half = tensor.half()  # 转换为半精度浮点型张量\n",
    "half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a010c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_t = tensor.int()  # 转换为整型张量\n",
    "int_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d4a02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flo = tensor.float()  # 转换为单精度浮点型张量\n",
    "flo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f2dc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42], dtype=torch.int16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = tensor.short()  # 转换为短整型张量\n",
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9305826a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42], dtype=torch.int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = tensor.char()  # 转换为字符型张量\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6ad7e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42], dtype=torch.uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt = tensor.byte()  # 转换为字节型张量\n",
    "bt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f46621",
   "metadata": {},
   "source": [
    "#### Numpy转换\n",
    "使用numpy方法把Tensor转换为ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ba8743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13345253 -0.22973956]\n",
      " [ 0.60256785  1.9548762 ]\n",
      " [-1.1720358   0.61694145]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((3, 2))\n",
    "numpy_a = a.numpy()  # 使用numpy方法把Tensor转换为ndarray\n",
    "print(numpy_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb242751",
   "metadata": {},
   "source": [
    "numpy转化为Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eb26829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1335, -0.2297],\n",
       "        [ 0.6026,  1.9549],\n",
       "        [-1.1720,  0.6169]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a = torch.from_numpy(numpy_a)  # 使用from_numpy方法把ndarray转换为Tensor\n",
    "torch_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba54c9c",
   "metadata": {},
   "source": [
    "Tensor和Numpy对象共享内存，所以他们之间转换得很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另一个也会随之改变。\n",
    "\n",
    "#### 设备间转换\n",
    "\n",
    "一般情况下可以使用`.cuda`方法将`tensor`移动到`gpu`，这步操作需要cuda设备支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb82728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_a = torch.rand(4, 3)  # 创建一个在CPU上的张量\n",
    "cpu_a.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57f9e3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量设备: mps:0\n",
      "张量类型: torch.mps.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# gpu_a = cpu_a.cuda()  # 把张量移动到GPU上 # 不使用于macOS\n",
    "# gpu_a.type()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "gpu_a = cpu_a.to(device)\n",
    "print(f\"张量设备: {gpu_a.device}\")\n",
    "print(f\"张量类型: {gpu_a.type()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44b91cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用.cuda方法把张量从GPU移动回CPU\n",
    "cpu_b = gpu_a.cpu()\n",
    "cpu_b.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117a6ff",
   "metadata": {},
   "source": [
    "如果我们有多GPU的情况，可以使用to方法来确定使用哪个设备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f034ef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用torch.cuda.is_available()来检查当前系统是否有可用的CUDA设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "gpu_b = cpu_b.to(device)  # 把张量移动到指定设备上\n",
    "gpu_b.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a39a1",
   "metadata": {},
   "source": [
    "#### 初始化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adc552c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2474, 0.2836, 0.1611],\n",
       "        [0.8726, 0.2797, 0.6178],\n",
       "        [0.3280, 0.7531, 0.0917],\n",
       "        [0.6948, 0.9211, 0.6420],\n",
       "        [0.5221, 0.6147, 0.1789]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd = torch.rand(5, 3)  # 生成一个5x3的随机张量\n",
    "rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f399664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = torch.ones(2, 2) # 生成一个2x2的全1张量\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b3bd018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero = torch.zeros(2, 2)  # 生成一个2x2的全0张量\n",
    "zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fc11ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye = torch.eye(2, 2) # 生成一个2x2的单位矩阵\n",
    "eye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185192c",
   "metadata": {},
   "source": [
    "#### 常用方法\n",
    "\n",
    "PyTorch中对张量的操作api和NumPy非常相似，如果熟悉NumPy中的操作，那么他们二者基本是一致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a4c67a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2978, -1.1216, -1.3120],\n",
      "        [-0.6444, -0.7867, -1.3209],\n",
      "        [ 0.0276,  1.8775,  1.3607]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "795342ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2978, -0.6444,  1.8775]) tensor([0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "max_value, max_idx = torch.max(x, dim = 1)  # 按行取最大值及其索引\n",
    "print(max_value, max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b63627e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6811, -0.0308, -1.2722])\n"
     ]
    }
   ],
   "source": [
    "sum_x = torch.sum(x, dim = 0)  # 按列求和\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6504d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5258, -1.7859, -0.3966],\n",
      "        [-0.7582,  0.1773, -1.6449],\n",
      "        [-0.3545,  4.2611,  2.3116]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randn(3, 3)\n",
    "z = x + y  # 张量加法\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85a3f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5258, -1.7859, -0.3966],\n",
      "        [-0.7582,  0.1773, -1.6449],\n",
      "        [-0.3545,  4.2611,  2.3116]])\n"
     ]
    }
   ],
   "source": [
    "x.add_(y)  # 张量加法，结果存储在x中\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677db3d7",
   "metadata": {},
   "source": [
    "### 使用PyTorch计算梯度数值\n",
    "\n",
    "PyTorch的`Autograd`模块实现了深度学习的算法中的向传播求导数，在张量（Tensor类）上的所有操作，`Autograd`都能为他们自动提供微分，简化了手动计算导数的复杂过程。\n",
    "\n",
    "要想通过Tensor类本身就支持了使用autograd功能，只需要设置`.requires_grad = True`\n",
    "\n",
    "Variable类中的`grad`和`grad_fn`属性已经整合进入了Tensor类中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1f029",
   "metadata": {},
   "source": [
    "#### Autograd\n",
    "在张量创建时，通过设置`requires_grad`表示为`True`来告诉PyTorch需要对该张量进行自动求导，PyTorch会记录该张量的每一步操作历史并自动计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "612446cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6070, 0.8575, 0.5657, 0.3939, 0.3134],\n",
      "        [0.3983, 0.8798, 0.3192, 0.5927, 0.4837],\n",
      "        [0.1083, 0.6101, 0.1454, 0.4246, 0.3370],\n",
      "        [0.5790, 0.2759, 0.5966, 0.7167, 0.4238],\n",
      "        [0.0926, 0.0801, 0.5709, 0.7173, 0.6505]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56ebf1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1099, 0.7146, 0.8885, 0.8838, 0.5491],\n",
       "        [0.6069, 0.0146, 0.2674, 0.3327, 0.7821],\n",
       "        [0.2905, 0.6269, 0.1861, 0.6863, 0.7453],\n",
       "        [0.5733, 0.8268, 0.8817, 0.7041, 0.6073],\n",
       "        [0.6877, 0.2613, 0.7635, 0.2653, 0.6642]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 5, requires_grad = True)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ebc61",
   "metadata": {},
   "source": [
    "PyTorch会自动追踪和记录对与张量的所有操作，当计算完成后调用`.backward()`方法自动计算梯度并且将计算结果保存到`grad`属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8bd6d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.6598, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.sum(x + y) # 计算x和y的和，并求和得到标量z\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6c7d6",
   "metadata": {},
   "source": [
    "在张量进行操作之后，`grad_fn`已经被赋予了一个新的函数，这个函数引用了一个创建了这个Tensor类的Function对象。Tensor和Function互相连接生成了一个非循环图，它记录并且编码了完整的计算历史。每个张量都有一个`.grad_fn`属性，如果这个张量时用户手动创建的，那么这个张量的`grad_fn`是`None`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e371d56",
   "metadata": {},
   "source": [
    "#### 简单的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b70dc9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "z.backward() # 对标量z进行反向传播，计算梯度\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23eb69",
   "metadata": {},
   "source": [
    "如果Tensor类表示的是一个标量，则不需要为`backward()`指定任何参数，但是如果它有更多的元素，则需要指定一个`gradient`参数，它是形状匹配的张量。以上的`z.backward()`相当于是`z.backward(torch.tensor(1.))`的简写。这种参数常出现在图像分类中的单标签分类，输出一个标量代表图像的标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6e903",
   "metadata": {},
   "source": [
    "#### 复杂的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e31c8cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8791, 1.4388, 1.2485, 1.0066, 1.5183],\n",
       "        [0.8075, 0.7772, 0.1792, 0.4368, 1.1118],\n",
       "        [0.2393, 0.4436, 0.3406, 0.5281, 0.4258],\n",
       "        [0.0288, 0.2033, 0.8205, 0.2889, 0.0615],\n",
       "        [0.6016, 1.0619, 0.1828, 0.7919, 0.2525]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 5, requires_grad = True)\n",
    "y = torch.rand(5, 5, requires_grad = True)\n",
    "z = x ** 2 + y ** 3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90db2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8742, 1.5366, 1.9541, 1.8168, 1.9006],\n",
      "        [1.7382, 1.7374, 0.6855, 1.3084, 1.5167],\n",
      "        [0.5984, 1.3275, 1.1492, 1.3208, 1.2971],\n",
      "        [0.3341, 0.8847, 1.7039, 1.0452, 0.2984],\n",
      "        [1.5370, 1.0643, 0.8282, 1.6255, 0.9354]])\n"
     ]
    }
   ],
   "source": [
    "# 对张量z进行反向传播，传入与z形状相同的全1张量 \n",
    "# 我们的返回值不是一个标量，所以需要输入一个大小相同的张量作为参数\n",
    "z.backward(torch.ones_like(x))  \n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc54ae",
   "metadata": {},
   "source": [
    "我们可以使用`with torch.no_grad()`上下文管理器临时禁制对已设置`requires_grad = True`的张量进行自动求导。这个方法在测试集计算准确率的时候会经常用到。\n",
    "\n",
    "例如:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4619f588",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'required_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequired_grad\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'required_grad'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((x + y*2).required_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f5634",
   "metadata": {},
   "source": [
    "使用`.no_grad()`进行嵌套后，代码不会跟踪历史记录，也就是说保存的这部分记录会减少内存的使用量并且会加快少许的运算速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66444496",
   "metadata": {},
   "source": [
    "#### Autograd过程解析\n",
    "\n",
    "为了说明PyTorch的自动求导原理，我们来尝试分析一下PyTorch的源代码，虽然PyTorch的Tensor和TensorBase都是使用CPP来实现的，但是可以使用一些Python的一些方法查看这些对象在Python的属性和状态。Python的`dir()`返回参数的属性，方法列表。`z`是一个Tensor变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f01bfa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mdir\u001b[39m(\u001b[43mz\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "dir(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd21977",
   "metadata": {},
   "source": [
    "返回很多，我们直接排除掉一些Python中特殊方法（以__开头和结束的）和私有方法（以_开头的），直接看几个比较主要的属性:`.is_leaf()`：记录是否是叶子节点。通过这个属性来确定这个变量的类型。\n",
    "\n",
    "在官方文档中所说的“graph leaves”，“leaf variables”，都是指像`x`，`y`这样的手动创建的、而非运算得到的变量，这些变量称为创建变量。像`z`这样的，是通过计算后得到的结果称为结果变量。\n",
    "\n",
    "一个变量是创建变量还是结果变量是通过`.is_leaf`来获取的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e175271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x.is_leaf = \" + str(x.is_leaf))\n",
    "print(\"z.is_leaf = \" + str(z.is_leaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fea5e",
   "metadata": {},
   "source": [
    "`x`是手动创建的，没有通过计算，所以被认为是一个叶子节点，也就是一个创建变量。而`z`是通过`x`与`y`的一系列计算得到的，所以不是叶子节点，也就是变量结果。\n",
    "\n",
    "为什么我们执行`z.backword()`方法会更新`x.grad()`和`y.grad`呢？\n",
    "\n",
    "`.grad_fn`属性记录的就是这部分操作，虽然`.backward()`方法也是CPP实现的，但是可以通过Python来进行简单的探索。\n",
    "\n",
    "`grad_fn`：记录并编码了完整的计算历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c90e392",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mz\u001b[49m.grad_fn\n",
      "\u001b[31mNameError\u001b[39m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e9991",
   "metadata": {},
   "source": [
    "`grad_fn`是一个`AddBackward0`类型的变量，`AddBackward0`这个类也是用CPP来写的，但是我们从名字里就能够大概知道，它是加法（ADD）的反向传播（Backward）。\n",
    "\n",
    "`next_function`就是`grad_fn`的精华"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d2bb73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mdir\u001b[39m(\u001b[43mz\u001b[49m.grad_fn)\n",
      "\u001b[31mNameError\u001b[39m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "dir(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115f8c42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mz\u001b[49m.graf_fn.next_functions\n",
      "\u001b[31mNameError\u001b[39m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "z.graf_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22d51f",
   "metadata": {},
   "source": [
    "`next_function`是一个tuple of tuple of PowBackward0 and int.\n",
    "\n",
    "**为什么是两个tuple？**\n",
    "\n",
    "因为我们的操作是`z = x**2 + y**3`。刚才的`AddBackward0`是相加，而前面的操作是乘方`PowBackward0`。tuple第一个元素就是x相关的操作记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c3c453",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m xg = \u001b[43mz\u001b[49m.grad_fn.next_functions[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mdir\u001b[39m(xg)\n",
      "\u001b[31mNameError\u001b[39m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "xg = z.grad_fn.next_functions[0][0]\n",
    "dir(xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ee4ebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x_leaf = \u001b[43mxg\u001b[49m.next_functions[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mtype\u001b[39m(x_leaf)\n",
      "\u001b[31mNameError\u001b[39m: name 'xg' is not defined"
     ]
    }
   ],
   "source": [
    "x_leaf = xg.next_functions[0][0]\n",
    "type(x_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08792f2e",
   "metadata": {},
   "source": [
    "在PyTorch的反向图计算中，`AccumulateGrad`类型代表的就是叶子节点类型，也就是计算图终止节点。`AccumulateGrad`类中有一个`.variable`属性指向叶子节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51d2a86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_leaf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mx_leaf\u001b[49m.variable\n",
      "\u001b[31mNameError\u001b[39m: name 'x_leaf' is not defined"
     ]
    }
   ],
   "source": [
    "x_leaf.variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33ed30",
   "metadata": {},
   "source": [
    "这个`.variable`属性就是我们的生成的变量`x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b202c814",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_leaf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mx_leaf.variable的id:\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(\u001b[43mx_leaf\u001b[49m.variable)))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mx的id:\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(x)))\n",
      "\u001b[31mNameError\u001b[39m: name 'x_leaf' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"x_leaf.variable的id:\" + str(id(x_leaf.variable)))\n",
    "print(\"x的id:\" + str(id(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b061c16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_leaf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mid\u001b[39m(\u001b[43mx_leaf\u001b[49m.variable)) == \u001b[38;5;28mid\u001b[39m(x)\n",
      "\u001b[31mNameError\u001b[39m: name 'x_leaf' is not defined"
     ]
    }
   ],
   "source": [
    "assert(id(x_leaf.variable)) == id(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1d9e6",
   "metadata": {},
   "source": [
    "这样整个规程就很清晰了：\n",
    "\n",
    "1. 当我们执行`z.backward()`的时候。这个操作将调用z里边的`grad_fn`这个属性，执行求导操作。\n",
    "2. 这个操作将遍历`grad_fn`的`next_functions`,然后分别取出里边的Function(AccumulatedGrad)，执行求导操作。这部分是一个递归的过程，直到最后类型为叶子节点。\n",
    "3. 计算出结果以后，将结果保存到他们对应的`variable`这个变量所引用的对象（x和y）的`grad`这个属性里面。\n",
    "4. 求导结束。所有的叶节点的`grad`变量都得到了相应的更新。\n",
    "\n",
    "最终当我们执行完`c.backward()`之后，a和b里面的grad值就得到了更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d6d6d",
   "metadata": {},
   "source": [
    "#### 扩展Autograd\n",
    "\n",
    "如果需要自定义autograd扩展新功能，就需要扩展Function类。因为Function使用autograd来计算结果和梯度，并对操作历史进行编码。在Function类中最重要的方法就是`forward()`和`backward()`，他们分别代表了前向传播和反向传播。\n",
    "\n",
    "一个自定义的Function需要以下三种方法：\n",
    "\n",
    "- `__init__(optional)`：如果这个操作需要额外的参数，则需要定义这个Function的构造函数，不需要的话可以忽略。\n",
    "- `forward()`：执行前向传播的计算代码。\n",
    "- `backward()`：反向传播时梯度计算的代码。参数的个数和`forward`返回值的个数一样，每个参数代表传回到此操作的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f6228e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Function \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMulConstant\u001b[39;00m(Function):\n\u001b[32m      4\u001b[39m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(ctx, tensor, constant):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from torch.autograd.function import Function \n",
    "\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        ctx.constant = constant    # 保存常量以便在反向传播中使用\n",
    "        return tensor * constant\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None # 返回梯度和None，因为constant不是张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259bad90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'troch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m a = \u001b[43mtroch\u001b[49m.rand(\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m, requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m b = MulConstant.apply(a, \u001b[32m5\u001b[39m) \n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[31mNameError\u001b[39m: name 'troch' is not defined"
     ]
    }
   ],
   "source": [
    "a = troch.rand(3, 3, requires_grad = True)\n",
    "b = MulConstant.apply(a, 5) \n",
    "print(\"a\" + str(a))\n",
    "print(\"b\" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e69ede7",
   "metadata": {},
   "source": [
    "反向传播，返回值不是标量，所以`backward`方法需要参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b56679e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mb\u001b[49m.backward(torch.ones_like(a)) \n",
      "\u001b[31mNameError\u001b[39m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "b.backward(torch.ones_like(a)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65076cad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m.grad\n",
      "\u001b[31mNameError\u001b[39m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
