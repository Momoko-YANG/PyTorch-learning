{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89a554e",
   "metadata": {},
   "source": [
    "#### PyTorch基础：张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dcde8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550ecd9",
   "metadata": {},
   "source": [
    "#### 张量（Tensor）\n",
    "Tensor是PyTorch里面基础的运算单位，与Numpy的ndarray相同，都表示的是一个多维的矩阵。与ndarray的最大区别就是，PyTorch的Tensor可以在GPU上运行，而Numpy的ndarray只能在CPU上运行，在GPU上运行大大加快了运算速度。\n",
    "\n",
    "下面我们生成一个简单的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a27aa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7641, 0.2276, 0.7648],\n",
       "        [0.0197, 0.9050, 0.5619]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3) # 生成一个2行3列的随机张量\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e74489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)  # 输出张量的形状\n",
    "print(x.size())  # 输出张量的形状，等价于x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063900e",
   "metadata": {},
   "source": [
    "张量（tensor）是一个定义在一些张量空间和一些对偶空间的笛卡尔积上的多重线性映射，其坐标是n维空间内，有n个分量的一种量，其中每个分量都是坐标的函数。而在坐标变换时，这些分量也依照某些规则作线性变换。r称为该张量的秩或阶（与矩阵的秩和阶均无关系）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f639dfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9627, 0.1141, 0.8486, 0.8356, 0.8587],\n",
       "          [0.5418, 0.8174, 0.6005, 0.0965, 0.8303],\n",
       "          [0.1391, 0.3076, 0.7895, 0.6155, 0.6628],\n",
       "          [0.3231, 0.5223, 0.5444, 0.8989, 0.4116]],\n",
       "\n",
       "         [[0.3931, 0.8159, 0.7216, 0.1741, 0.6903],\n",
       "          [0.8077, 0.0810, 0.6242, 0.7658, 0.6776],\n",
       "          [0.9745, 0.5951, 0.5533, 0.6079, 0.3307],\n",
       "          [0.7373, 0.2226, 0.8817, 0.2303, 0.9428]],\n",
       "\n",
       "         [[0.9879, 0.7443, 0.7721, 0.0296, 0.6624],\n",
       "          [0.3084, 0.1969, 0.3693, 0.1537, 0.1498],\n",
       "          [0.1185, 0.4198, 0.9368, 0.7222, 0.7226],\n",
       "          [0.6109, 0.5037, 0.3604, 0.0631, 0.8926]]],\n",
       "\n",
       "\n",
       "        [[[0.1489, 0.3011, 0.9575, 0.2981, 0.1518],\n",
       "          [0.9751, 0.5308, 0.8342, 0.2133, 0.4051],\n",
       "          [0.9869, 0.1186, 0.4335, 0.0045, 0.6526],\n",
       "          [0.3630, 0.0477, 0.9128, 0.6366, 0.6189]],\n",
       "\n",
       "         [[0.1986, 0.3052, 0.9606, 0.3996, 0.4836],\n",
       "          [0.4570, 0.1778, 0.9988, 0.1658, 0.7575],\n",
       "          [0.7717, 0.8957, 0.0051, 0.0272, 0.0636],\n",
       "          [0.5142, 0.8078, 0.3917, 0.5678, 0.8219]],\n",
       "\n",
       "         [[0.7913, 0.1670, 0.1333, 0.0792, 0.6229],\n",
       "          [0.7346, 0.3928, 0.4827, 0.0359, 0.7416],\n",
       "          [0.6001, 0.6131, 0.1578, 0.6034, 0.6070],\n",
       "          [0.3516, 0.2786, 0.6116, 0.0037, 0.6802]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(2,3,4,5)  # 生成一个2x3x4x5的随机张量\n",
    "print(y.size())  # 输出张量的形状\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bebaff",
   "metadata": {},
   "source": [
    "在同构的意义下，第零阶张量（r=0）为标量（Scalar），第一阶张量（r=1）为向量（Vector），第二阶张量（r=2）则为矩阵（Matrix），第三阶以上统称为多维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed71b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = torch.tensor(5)  # 标量\n",
    "print(scalar)\n",
    "scalar.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33f3e9",
   "metadata": {},
   "source": [
    " 对于标量，我们可以直接用`.item()`从中取出其对应的python对象的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6882fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()  # 取出标量的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de2cb88",
   "metadata": {},
   "source": [
    "特别的：如果张量中只有一个元素的tensor也可以调用`tensor.item`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55ee99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([42])  # 只有一个元素的张量\n",
    "print(tensor)\n",
    "tensor.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e537a46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.item()  # 取出张量中唯一元素的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4a66c",
   "metadata": {},
   "source": [
    "#### 基本类型\n",
    "\n",
    "Tensor的基本数据类型有五种：\n",
    "\n",
    "- 32位浮点型：`torch.FloatTensor`（默认）\n",
    "- 64位整型：`torch.LongTensor`\n",
    "- 32位整型：`torch.IntTensor`\n",
    "- 16位整型：`torch.ShortTensor`\n",
    "- 64位浮点型：`torch.DoubleTensor`\n",
    "\n",
    "除以上数字类型外，还有`byte`和 `chart`型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1724fe4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long = tensor.long()  # 转换为长整型张量\n",
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f26b1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.], dtype=torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half = tensor.half()  # 转换为半精度浮点型张量\n",
    "half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a010c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_t = tensor.int()  # 转换为整型张量\n",
    "int_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d4a02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flo = tensor.float()  # 转换为单精度浮点型张量\n",
    "flo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f2dc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42], dtype=torch.int16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = tensor.short()  # 转换为短整型张量\n",
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9305826a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42], dtype=torch.int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = tensor.char()  # 转换为字符型张量\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6ad7e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42], dtype=torch.uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt = tensor.byte()  # 转换为字节型张量\n",
    "bt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f46621",
   "metadata": {},
   "source": [
    "#### Numpy转换\n",
    "使用numpy方法把Tensor转换为ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ba8743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13345253 -0.22973956]\n",
      " [ 0.60256785  1.9548762 ]\n",
      " [-1.1720358   0.61694145]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((3, 2))\n",
    "numpy_a = a.numpy()  # 使用numpy方法把Tensor转换为ndarray\n",
    "print(numpy_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb242751",
   "metadata": {},
   "source": [
    "numpy转化为Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eb26829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1335, -0.2297],\n",
       "        [ 0.6026,  1.9549],\n",
       "        [-1.1720,  0.6169]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a = torch.from_numpy(numpy_a)  # 使用from_numpy方法把ndarray转换为Tensor\n",
    "torch_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba54c9c",
   "metadata": {},
   "source": [
    "Tensor和Numpy对象共享内存，所以他们之间转换得很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另一个也会随之改变。\n",
    "\n",
    "#### 设备间转换\n",
    "\n",
    "一般情况下可以使用`.cuda`方法将`tensor`移动到`gpu`，这步操作需要cuda设备支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb82728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_a = torch.rand(4, 3)  # 创建一个在CPU上的张量\n",
    "cpu_a.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57f9e3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量设备: mps:0\n",
      "张量类型: torch.mps.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# gpu_a = cpu_a.cuda()  # 把张量移动到GPU上 # 不使用于macOS\n",
    "# gpu_a.type()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "gpu_a = cpu_a.to(device)\n",
    "print(f\"张量设备: {gpu_a.device}\")\n",
    "print(f\"张量类型: {gpu_a.type()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44b91cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用.cuda方法把张量从GPU移动回CPU\n",
    "cpu_b = gpu_a.cpu()\n",
    "cpu_b.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117a6ff",
   "metadata": {},
   "source": [
    "如果我们有多GPU的情况，可以使用to方法来确定使用哪个设备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f034ef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用torch.cuda.is_available()来检查当前系统是否有可用的CUDA设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "gpu_b = cpu_b.to(device)  # 把张量移动到指定设备上\n",
    "gpu_b.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a39a1",
   "metadata": {},
   "source": [
    "#### 初始化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adc552c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2474, 0.2836, 0.1611],\n",
       "        [0.8726, 0.2797, 0.6178],\n",
       "        [0.3280, 0.7531, 0.0917],\n",
       "        [0.6948, 0.9211, 0.6420],\n",
       "        [0.5221, 0.6147, 0.1789]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd = torch.rand(5, 3)  # 生成一个5x3的随机张量\n",
    "rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f399664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = torch.ones(2, 2) # 生成一个2x2的全1张量\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b3bd018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero = torch.zeros(2, 2)  # 生成一个2x2的全0张量\n",
    "zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fc11ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye = torch.eye(2, 2) # 生成一个2x2的单位矩阵\n",
    "eye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185192c",
   "metadata": {},
   "source": [
    "#### 常用方法\n",
    "\n",
    "PyTorch中对张量的操作api和NumPy非常相似，如果熟悉NumPy中的操作，那么他们二者基本是一致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a4c67a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2978, -1.1216, -1.3120],\n",
      "        [-0.6444, -0.7867, -1.3209],\n",
      "        [ 0.0276,  1.8775,  1.3607]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "795342ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2978, -0.6444,  1.8775]) tensor([0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "max_value, max_idx = torch.max(x, dim = 1)  # 按行取最大值及其索引\n",
    "print(max_value, max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b63627e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6811, -0.0308, -1.2722])\n"
     ]
    }
   ],
   "source": [
    "sum_x = torch.sum(x, dim = 0)  # 按列求和\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6504d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5258, -1.7859, -0.3966],\n",
      "        [-0.7582,  0.1773, -1.6449],\n",
      "        [-0.3545,  4.2611,  2.3116]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randn(3, 3)\n",
    "z = x + y  # 张量加法\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85a3f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5258, -1.7859, -0.3966],\n",
      "        [-0.7582,  0.1773, -1.6449],\n",
      "        [-0.3545,  4.2611,  2.3116]])\n"
     ]
    }
   ],
   "source": [
    "x.add_(y)  # 张量加法，结果存储在x中\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677db3d7",
   "metadata": {},
   "source": [
    "### 使用PyTorch计算梯度数值\n",
    "\n",
    "PyTorch的`Autograd`模块实现了深度学习的算法中的向传播求导数，在张量（Tensor类）上的所有操作，`Autograd`都能为他们自动提供微分，简化了手动计算导数的复杂过程。\n",
    "\n",
    "要想通过Tensor类本身就支持了使用autograd功能，只需要设置`.requires_grad = True`\n",
    "\n",
    "Variable类中的`grad`和`grad_fn`属性已经整合进入了Tensor类中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1f029",
   "metadata": {},
   "source": [
    "#### Autograd\n",
    "在张量创建时，通过设置`requires_grad`表示为`True`来告诉PyTorch需要对该张量进行自动求导，PyTorch会记录该张量的每一步操作历史并自动计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "612446cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6070, 0.8575, 0.5657, 0.3939, 0.3134],\n",
      "        [0.3983, 0.8798, 0.3192, 0.5927, 0.4837],\n",
      "        [0.1083, 0.6101, 0.1454, 0.4246, 0.3370],\n",
      "        [0.5790, 0.2759, 0.5966, 0.7167, 0.4238],\n",
      "        [0.0926, 0.0801, 0.5709, 0.7173, 0.6505]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56ebf1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1099, 0.7146, 0.8885, 0.8838, 0.5491],\n",
       "        [0.6069, 0.0146, 0.2674, 0.3327, 0.7821],\n",
       "        [0.2905, 0.6269, 0.1861, 0.6863, 0.7453],\n",
       "        [0.5733, 0.8268, 0.8817, 0.7041, 0.6073],\n",
       "        [0.6877, 0.2613, 0.7635, 0.2653, 0.6642]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 5, requires_grad = True)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ebc61",
   "metadata": {},
   "source": [
    "PyTorch会自动追踪和记录对与张量的所有操作，当计算完成后调用`.backward()`方法自动计算梯度并且将计算结果保存到`grad`属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8bd6d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.6598, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.sum(x + y) # 计算x和y的和，并求和得到标量z\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6c7d6",
   "metadata": {},
   "source": [
    "在张量进行操作之后，`grad_fn`已经被赋予了一个新的函数，这个函数引用了一个创建了这个Tensor类的Function对象。Tensor和Function互相连接生成了一个非循环图，它记录并且编码了完整的计算历史。每个张量都有一个`.grad_fn`属性，如果这个张量时用户手动创建的，那么这个张量的`grad_fn`是`None`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e371d56",
   "metadata": {},
   "source": [
    "#### 简单的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b70dc9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "z.backward() # 对标量z进行反向传播，计算梯度\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23eb69",
   "metadata": {},
   "source": [
    "如果Tensor类表示的是一个标量，则不需要为`backward()`指定任何参数，但是如果它有更多的元素，则需要指定一个`gradient`参数，它是形状匹配的张量。以上的`z.backward()`相当于是`z.backward(torch.tensor(1.))`的简写。这种参数常出现在图像分类中的单标签分类，输出一个标量代表图像的标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6e903",
   "metadata": {},
   "source": [
    "#### 复杂的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e31c8cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8791, 1.4388, 1.2485, 1.0066, 1.5183],\n",
       "        [0.8075, 0.7772, 0.1792, 0.4368, 1.1118],\n",
       "        [0.2393, 0.4436, 0.3406, 0.5281, 0.4258],\n",
       "        [0.0288, 0.2033, 0.8205, 0.2889, 0.0615],\n",
       "        [0.6016, 1.0619, 0.1828, 0.7919, 0.2525]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 5, requires_grad = True)\n",
    "y = torch.rand(5, 5, requires_grad = True)\n",
    "z = x ** 2 + y ** 3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90db2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8742, 1.5366, 1.9541, 1.8168, 1.9006],\n",
      "        [1.7382, 1.7374, 0.6855, 1.3084, 1.5167],\n",
      "        [0.5984, 1.3275, 1.1492, 1.3208, 1.2971],\n",
      "        [0.3341, 0.8847, 1.7039, 1.0452, 0.2984],\n",
      "        [1.5370, 1.0643, 0.8282, 1.6255, 0.9354]])\n"
     ]
    }
   ],
   "source": [
    "# 对张量z进行反向传播，传入与z形状相同的全1张量 \n",
    "# 我们的返回值不是一个标量，所以需要输入一个大小相同的张量作为参数\n",
    "z.backward(torch.ones_like(x))  \n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc54ae",
   "metadata": {},
   "source": [
    "我们可以使用`with torch.no_grad()`上下文管理器临时禁制对已设置`requires_grad = True`的张量进行自动求导。这个方法在测试集计算准确率的时候会经常用到。\n",
    "\n",
    "例如:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4619f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((x + y*2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f5634",
   "metadata": {},
   "source": [
    "使用`.no_grad()`进行嵌套后，代码不会跟踪历史记录，也就是说保存的这部分记录会减少内存的使用量并且会加快少许的运算速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66444496",
   "metadata": {},
   "source": [
    "#### Autograd过程解析\n",
    "\n",
    "为了说明PyTorch的自动求导原理，我们来尝试分析一下PyTorch的源代码，虽然PyTorch的Tensor和TensorBase都是使用CPP来实现的，但是可以使用一些Python的一些方法查看这些对象在Python的属性和状态。Python的`dir()`返回参数的属性，方法列表。`z`是一个Tensor变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f01bfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__dlpack__',\n",
       " '__dlpack_device__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rlshift__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rrshift__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__torch_dispatch__',\n",
       " '__torch_function__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_addmm_activation',\n",
       " '_autocast_to_full_precision',\n",
       " '_autocast_to_reduced_precision',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_clear_non_serializable_cached_data',\n",
       " '_coalesced_',\n",
       " '_conj',\n",
       " '_conj_physical',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_fix_weakref',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_has_symbolic_sizes_strides',\n",
       " '_indices',\n",
       " '_is_all_true',\n",
       " '_is_any_true',\n",
       " '_is_view',\n",
       " '_is_zerotensor',\n",
       " '_lazy_clone',\n",
       " '_make_subclass',\n",
       " '_make_wrapper_subclass',\n",
       " '_neg_view',\n",
       " '_nested_tensor_size',\n",
       " '_nested_tensor_storage_offsets',\n",
       " '_nested_tensor_strides',\n",
       " '_nnz',\n",
       " '_post_accumulate_grad_hooks',\n",
       " '_python_dispatch',\n",
       " '_reduce_ex_internal',\n",
       " '_rev_view_func_unsafe',\n",
       " '_sparse_mask_projection',\n",
       " '_to_dense',\n",
       " '_to_sparse',\n",
       " '_to_sparse_bsc',\n",
       " '_to_sparse_bsr',\n",
       " '_to_sparse_csc',\n",
       " '_to_sparse_csr',\n",
       " '_typed_storage',\n",
       " '_update_names',\n",
       " '_use_count',\n",
       " '_values',\n",
       " '_version',\n",
       " '_view_func',\n",
       " '_view_func_unsafe',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'absolute',\n",
       " 'absolute_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'acosh',\n",
       " 'acosh_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'adjoint',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'aminmax',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'arccos',\n",
       " 'arccos_',\n",
       " 'arccosh',\n",
       " 'arccosh_',\n",
       " 'arcsin',\n",
       " 'arcsin_',\n",
       " 'arcsinh',\n",
       " 'arcsinh_',\n",
       " 'arctan',\n",
       " 'arctan2',\n",
       " 'arctan2_',\n",
       " 'arctan_',\n",
       " 'arctanh',\n",
       " 'arctanh_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'argwhere',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_strided_scatter',\n",
       " 'as_subclass',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'asinh',\n",
       " 'asinh_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'atanh',\n",
       " 'atanh_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_and_',\n",
       " 'bitwise_left_shift',\n",
       " 'bitwise_left_shift_',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_or',\n",
       " 'bitwise_or_',\n",
       " 'bitwise_right_shift',\n",
       " 'bitwise_right_shift_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_to',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ccol_indices',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'cfloat',\n",
       " 'chalf',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clip',\n",
       " 'clip_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'col_indices',\n",
       " 'conj',\n",
       " 'conj_physical',\n",
       " 'conj_physical_',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'copysign',\n",
       " 'copysign_',\n",
       " 'corrcoef',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'crow_indices',\n",
       " 'cuda',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumprod_',\n",
       " 'cumsum',\n",
       " 'cumsum_',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'deg2rad',\n",
       " 'deg2rad_',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diagonal_scatter',\n",
       " 'diff',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dim_order',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'divide',\n",
       " 'divide_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'exp2_',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'fix',\n",
       " 'fix_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float',\n",
       " 'float_power',\n",
       " 'float_power_',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'floor_divide',\n",
       " 'floor_divide_',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frexp',\n",
       " 'gather',\n",
       " 'gcd',\n",
       " 'gcd_',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'greater',\n",
       " 'greater_',\n",
       " 'greater_equal',\n",
       " 'greater_equal_',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'heaviside',\n",
       " 'heaviside_',\n",
       " 'histc',\n",
       " 'histogram',\n",
       " 'hsplit',\n",
       " 'hypot',\n",
       " 'hypot_',\n",
       " 'i0',\n",
       " 'i0_',\n",
       " 'igamma',\n",
       " 'igamma_',\n",
       " 'igammac',\n",
       " 'igammac_',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_reduce',\n",
       " 'index_reduce_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'inner',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'ipu',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_conj',\n",
       " 'is_contiguous',\n",
       " 'is_cpu',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_inference',\n",
       " 'is_ipu',\n",
       " 'is_leaf',\n",
       " 'is_maia',\n",
       " 'is_meta',\n",
       " 'is_mkldnn',\n",
       " 'is_mps',\n",
       " 'is_mtia',\n",
       " 'is_neg',\n",
       " 'is_nested',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'is_sparse_csr',\n",
       " 'is_vulkan',\n",
       " 'is_xla',\n",
       " 'is_xpu',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'istft',\n",
       " 'item',\n",
       " 'itemsize',\n",
       " 'kron',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'lcm',\n",
       " 'lcm_',\n",
       " 'ldexp',\n",
       " 'ldexp_',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'less',\n",
       " 'less_',\n",
       " 'less_equal',\n",
       " 'less_equal_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logcumsumexp',\n",
       " 'logdet',\n",
       " 'logical_and',\n",
       " 'logical_and_',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_or',\n",
       " 'logical_or_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logit',\n",
       " 'logit_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'mH',\n",
       " 'mT',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_exp',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'module_load',\n",
       " 'moveaxis',\n",
       " 'movedim',\n",
       " 'msort',\n",
       " 'mtia',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'multiply_',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'nan_to_num',\n",
       " 'nan_to_num_',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanquantile',\n",
       " 'nansum',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'negative',\n",
       " 'negative_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_empty_strided',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nextafter',\n",
       " 'nextafter_',\n",
       " 'nonzero',\n",
       " 'nonzero_static',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'not_equal',\n",
       " 'not_equal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'outer',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'quantile',\n",
       " 'rad2deg',\n",
       " 'rad2deg_',\n",
       " 'random_',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'register_post_accumulate_grad_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'resize_as_sparse_',\n",
       " 'resolve_conj',\n",
       " 'resolve_neg',\n",
       " 'retain_grad',\n",
       " 'retains_grad',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'row_indices',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'scatter_reduce',\n",
       " 'scatter_reduce_',\n",
       " 'select',\n",
       " 'select_scatter',\n",
       " 'set_',\n",
       " 'sgn',\n",
       " 'sgn_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'signbit',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinc',\n",
       " 'sinc_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slice_inverse',\n",
       " 'slice_scatter',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'square',\n",
       " 'square_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'subtract',\n",
       " 'subtract_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'swapaxes',\n",
       " 'swapaxes_',\n",
       " 'swapdims',\n",
       " 'swapdims_',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'take_along_dim',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor_split',\n",
       " 'tile',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_padded_tensor',\n",
       " 'to_sparse',\n",
       " 'to_sparse_bsc',\n",
       " 'to_sparse_bsr',\n",
       " 'to_sparse_coo',\n",
       " 'to_sparse_csc',\n",
       " 'to_sparse_csr',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'true_divide',\n",
       " 'true_divide_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsafe_chunk',\n",
       " 'unsafe_split',\n",
       " 'unsafe_split_with_sizes',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'untyped_storage',\n",
       " 'values',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'vsplit',\n",
       " 'where',\n",
       " 'xlogy',\n",
       " 'xlogy_',\n",
       " 'xpu',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd21977",
   "metadata": {},
   "source": [
    "返回很多，我们直接排除掉一些Python中特殊方法（以__开头和结束的）和私有方法（以_开头的），直接看几个比较主要的属性:`.is_leaf()`：记录是否是叶子节点。通过这个属性来确定这个变量的类型。\n",
    "\n",
    "在官方文档中所说的“graph leaves”，“leaf variables”，都是指像`x`，`y`这样的手动创建的、而非运算得到的变量，这些变量称为创建变量。像`z`这样的，是通过计算后得到的结果称为结果变量。\n",
    "\n",
    "一个变量是创建变量还是结果变量是通过`.is_leaf`来获取的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e175271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.is_leaf = True\n",
      "z.is_leaf = False\n"
     ]
    }
   ],
   "source": [
    "print(\"x.is_leaf = \" + str(x.is_leaf))\n",
    "print(\"z.is_leaf = \" + str(z.is_leaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fea5e",
   "metadata": {},
   "source": [
    "`x`是手动创建的，没有通过计算，所以被认为是一个叶子节点，也就是一个创建变量。而`z`是通过`x`与`y`的一系列计算得到的，所以不是叶子节点，也就是变量结果。\n",
    "\n",
    "为什么我们执行`z.backword()`方法会更新`x.grad()`和`y.grad`呢？\n",
    "\n",
    "`.grad_fn`属性记录的就是这部分操作，虽然`.backward()`方法也是CPP实现的，但是可以通过Python来进行简单的探索。\n",
    "\n",
    "`grad_fn`：记录并编码了完整的计算历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c90e392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x12b52cca0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e9991",
   "metadata": {},
   "source": [
    "`grad_fn`是一个`AddBackward0`类型的变量，`AddBackward0`这个类也是用CPP来写的，但是我们从名字里就能够大概知道，它是加法（ADD）的反向传播（Backward）。\n",
    "\n",
    "`next_function`就是`grad_fn`的精华"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5d2bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_input_metadata',\n",
       " '_register_hook_dict',\n",
       " '_saved_alpha',\n",
       " '_sequence_nr',\n",
       " '_set_sequence_nr',\n",
       " 'metadata',\n",
       " 'name',\n",
       " 'next_functions',\n",
       " 'register_hook',\n",
       " 'register_prehook',\n",
       " 'requires_grad']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "115f8c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<PowBackward0 at 0x105430eb0>, 0), (<PowBackward0 at 0x105430640>, 0))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22d51f",
   "metadata": {},
   "source": [
    "`next_function`是一个tuple of tuple of PowBackward0 and int.\n",
    "\n",
    "**为什么是两个tuple？**\n",
    "\n",
    "因为我们的操作是`z = x**2 + y**3`。刚才的`AddBackward0`是相加，而前面的操作是乘方`PowBackward0`。tuple第一个元素就是x相关的操作记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3c3c453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_input_metadata',\n",
       " '_raw_saved_self',\n",
       " '_register_hook_dict',\n",
       " '_saved_exponent',\n",
       " '_saved_self',\n",
       " '_sequence_nr',\n",
       " '_set_sequence_nr',\n",
       " 'metadata',\n",
       " 'name',\n",
       " 'next_functions',\n",
       " 'register_hook',\n",
       " 'register_prehook',\n",
       " 'requires_grad']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg = z.grad_fn.next_functions[0][0]\n",
    "dir(xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4ee4ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AccumulateGrad"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_leaf = xg.next_functions[0][0]\n",
    "type(x_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08792f2e",
   "metadata": {},
   "source": [
    "在PyTorch的反向图计算中，`AccumulateGrad`类型代表的就是叶子节点类型，也就是计算图终止节点。`AccumulateGrad`类中有一个`.variable`属性指向叶子节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d51d2a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9371, 0.7683, 0.9770, 0.9084, 0.9503],\n",
       "        [0.8691, 0.8687, 0.3428, 0.6542, 0.7584],\n",
       "        [0.2992, 0.6637, 0.5746, 0.6604, 0.6486],\n",
       "        [0.1671, 0.4424, 0.8520, 0.5226, 0.1492],\n",
       "        [0.7685, 0.5322, 0.4141, 0.8128, 0.4677]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_leaf.variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33ed30",
   "metadata": {},
   "source": [
    "这个`.variable`属性就是我们的生成的变量`x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b202c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_leaf.variable的id:5020535280\n",
      "x的id:5020535280\n"
     ]
    }
   ],
   "source": [
    "print(\"x_leaf.variable的id:\" + str(id(x_leaf.variable)))\n",
    "print(\"x的id:\" + str(id(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b061c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(id(x_leaf.variable)) == id(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1d9e6",
   "metadata": {},
   "source": [
    "这样整个规程就很清晰了：\n",
    "\n",
    "1. 当我们执行`z.backward()`的时候。这个操作将调用z里边的`grad_fn`这个属性，执行求导操作。\n",
    "2. 这个操作将遍历`grad_fn`的`next_functions`,然后分别取出里边的Function(AccumulatedGrad)，执行求导操作。这部分是一个递归的过程，直到最后类型为叶子节点。\n",
    "3. 计算出结果以后，将结果保存到他们对应的`variable`这个变量所引用的对象（x和y）的`grad`这个属性里面。\n",
    "4. 求导结束。所有的叶节点的`grad`变量都得到了相应的更新。\n",
    "\n",
    "最终当我们执行完`c.backward()`之后，a和b里面的grad值就得到了更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d6d6d",
   "metadata": {},
   "source": [
    "#### 扩展Autograd\n",
    "\n",
    "如果需要自定义autograd扩展新功能，就需要扩展Function类。因为Function使用autograd来计算结果和梯度，并对操作历史进行编码。在Function类中最重要的方法就是`forward()`和`backward()`，他们分别代表了前向传播和反向传播。\n",
    "\n",
    "一个自定义的Function需要以下三种方法：\n",
    "\n",
    "- `__init__(optional)`：如果这个操作需要额外的参数，则需要定义这个Function的构造函数，不需要的话可以忽略。\n",
    "- `forward()`：执行前向传播的计算代码。\n",
    "- `backward()`：反向传播时梯度计算的代码。参数的个数和`forward`返回值的个数一样，每个参数代表传回到此操作的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6f6228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.function import Function \n",
    "\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        ctx.constant = constant    # 保存常量以便在反向传播中使用\n",
    "        return tensor * constant\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None # 返回梯度和None，因为constant不是张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "259bad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atensor([[0.2810, 0.8864, 0.7907],\n",
      "        [0.0418, 0.9751, 0.9946],\n",
      "        [0.4780, 0.8575, 0.1317]], requires_grad=True)\n",
      "btensor([[1.4051, 4.4319, 3.9535],\n",
      "        [0.2090, 4.8756, 4.9731],\n",
      "        [2.3900, 4.2875, 0.6586]], grad_fn=<MulConstantBackward>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 3, requires_grad = True)\n",
    "b = MulConstant.apply(a, 5) \n",
    "print(\"a\" + str(a))\n",
    "print(\"b\" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e69ede7",
   "metadata": {},
   "source": [
    "反向传播，返回值不是标量，所以`backward`方法需要参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b56679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward(torch.ones_like(a)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65076cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
